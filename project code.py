# -*- coding: utf-8 -*-
"""Copy of MINI_PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2yTmo3SwtdIjgLonKbmd8hfJNVDNLW1
"""

import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import spacy
import matplotlib.pyplot as plt

# Load NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Load dataset
tweets_df = pd.read_csv('Tweets.csv')

# Function for sentiment analysis using TextBlob and spaCy
def perform_sentiment_analysis(text):
    # TextBlob sentiment analysis
    blob = TextBlob(text)
    sentiment_polarity = blob.sentiment.polarity

    # Scale sentiment polarity to a range of 0 to 100
    sentiment_score = (sentiment_polarity + 1) * 50

    # spaCy sentiment analysis and suggestions for improvement
    doc = nlp(text)
    suggestions = []
    for token in doc:
        if token.pos_ in ['ADJ', 'ADV', 'VERB'] and token.sentiment < 0:
            suggestions.append(f"Improve {token.pos_}: {token.text}")

    return sentiment_score, suggestions

# Apply sentiment analysis function to each tweet
tweets_df['sentiment_score'], tweets_df['sentiment_suggestions'] = zip(*tweets_df['text'].apply(perform_sentiment_analysis))

# Tokenization and padding for neural network
max_words = 20000
max_len = 100

tokenizer = Tokenizer(num_words=max_words, lower=True)
tokenizer.fit_on_texts(tweets_df['text'].values)
sequences = tokenizer.texts_to_sequences(tweets_df['text'].values)
data = pad_sequences(sequences, maxlen=max_len)

# Prepare labels
labels = pd.get_dummies(tweets_df['airline_sentiment']).values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# CNN Model
cnn_model = Sequential()
cnn_model.add(Embedding(max_words, 128, input_length=max_len))
cnn_model.add(Conv1D(64, 5, activation='relu'))
cnn_model.add(MaxPooling1D(pool_size=4))
cnn_model.add(GlobalMaxPooling1D())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(3, activation='softmax'))

cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the CNN model
history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)

# Evaluate the CNN model
cnn_pred = cnn_model.predict(X_test)
cnn_pred_classes = np.argmax(cnn_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)
cnn_accuracy = accuracy_score(y_test_classes, cnn_pred_classes)
cnn_classification_report = classification_report(y_test_classes, cnn_pred_classes, target_names=['negative', 'neutral', 'positive'])

# RNN Model
rnn_model = Sequential()
rnn_model.add(Embedding(max_words, 128, input_length=max_len))
rnn_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
rnn_model.add(Dense(3, activation='softmax'))

rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the RNN model
history_rnn = rnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)

# Evaluate the RNN model
rnn_pred = rnn_model.predict(X_test)
rnn_pred_classes = np.argmax(rnn_pred, axis=1)
rnn_accuracy = accuracy_score(y_test_classes, rnn_pred_classes)
rnn_classification_report = classification_report(y_test_classes, rnn_pred_classes, target_names=['negative', 'neutral', 'positive'])

# Plotting Accuracy and Loss Curves
def plot_training_history(history, model_name):
    plt.figure(figsize=(12, 6))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot CNN training history
plot_training_history(history_cnn, 'CNN')

# Plot RNN training history
plot_training_history(history_rnn, 'RNN')

# Summary Table
summary_table = pd.DataFrame({
    'Model': ['CNN', 'RNN'],
    'Accuracy': [cnn_accuracy, rnn_accuracy],
    'Classification Report': [cnn_classification_report, rnn_classification_report]
})

print("Summary Table:")
print(summary_table)

# Print the first few rows of the updated dataset with sentiment analysis results
print("\nUpdated Dataset with Sentiment Analysis Results:")
print(tweets_df.head())